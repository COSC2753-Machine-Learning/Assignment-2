{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore Warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import os, sys\n",
    "import torch\n",
    "import torchvision\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Add the module path to the sys.path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local helper files\n",
    "from utils.dataset import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CUDA devices: 1\n",
      "Device 0: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available CUDA devices: {num_gpus}\")\n",
    "    \n",
    "    # Print the name of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "# Set the device\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the shape of the inputs for our network\n",
    "IMG_SIZE = 120\n",
    "CROP_SIZE = 110\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the batch size, number of epochs, and the size of the chunks\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 2\n",
    "CHUNK_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predefined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair Dataset\n",
    "class FurniturePairDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        self.data_df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.color_histograms = []\n",
    "        self.edge_histograms = []\n",
    "        self._load_images_and_features()\n",
    "\n",
    "    def _load_images_and_features(self):\n",
    "        for idx in range(len(self.data_df)):\n",
    "            try: \n",
    "                img_name = os.path.join(self.root_dir, self.data_df.iloc[idx, 0])\n",
    "                image = Image.open(img_name)\n",
    "                self.images.append(image)\n",
    "                self.color_histograms.append(self._extract_color_histogram(image))\n",
    "                self.edge_histograms.append(self._extract_edge_histogram(image))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "\n",
    "    def _extract_color_histogram(self, image):\n",
    "        image = np.array(image.convert(\"RGB\"))\n",
    "        hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "        cv2.normalize(hist, hist)\n",
    "        return hist.flatten()\n",
    "\n",
    "    def _extract_edge_histogram(self, image):\n",
    "        image = np.array(image.convert(\"L\"))\n",
    "        edges = cv2.Canny(image, 100, 200)\n",
    "        hist = cv2.calcHist([edges], [0], None, [256], [0, 256])\n",
    "        cv2.normalize(hist, hist)\n",
    "        return hist.flatten()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df) * 2  # Each image will appear in one positive and one negative pair\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_idx = idx // 2\n",
    "        anchor_image = self.images[anchor_idx]\n",
    "        anchor_color_hist = self.color_histograms[anchor_idx]\n",
    "        anchor_edge_hist = self.edge_histograms[anchor_idx]\n",
    "\n",
    "        if self.transform:\n",
    "            anchor_image = self.transform(anchor_image)\n",
    "\n",
    "        if idx % 2 == 0:\n",
    "            similar_idx = self._find_most_similar(anchor_idx, anchor_color_hist)\n",
    "            similar_image = self.images[similar_idx]\n",
    "\n",
    "            if self.transform:\n",
    "                similar_image = self.transform(similar_image)\n",
    "\n",
    "            return (anchor_image, similar_image), 1\n",
    "        else:\n",
    "            dissimilar_idx = self._find_most_dissimilar(anchor_idx, anchor_edge_hist)\n",
    "            dissimilar_image = self.images[dissimilar_idx]\n",
    "\n",
    "            if self.transform:\n",
    "                dissimilar_image = self.transform(dissimilar_image)\n",
    "\n",
    "            return (anchor_image, dissimilar_image), 0\n",
    "\n",
    "    def _find_most_similar(self, anchor_idx, anchor_hist):\n",
    "        max_similarity = -1\n",
    "        similar_idx = -1\n",
    "        for idx in range(len(self.data_df)):\n",
    "            if idx == anchor_idx:\n",
    "                continue\n",
    "            hist = self.color_histograms[idx]\n",
    "            similarity = cv2.compareHist(anchor_hist, hist, cv2.HISTCMP_CORREL)\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                similar_idx = idx\n",
    "        return similar_idx\n",
    "\n",
    "    def _find_most_dissimilar(self, anchor_idx, anchor_hist):\n",
    "        min_similarity = 1\n",
    "        dissimilar_idx = -1\n",
    "        for idx in range(len(self.data_df)):\n",
    "            if idx == anchor_idx:\n",
    "                continue\n",
    "            hist = self.edge_histograms[idx]\n",
    "            similarity = cv2.compareHist(anchor_hist, hist, cv2.HISTCMP_CORREL)\n",
    "            if similarity < min_similarity:\n",
    "                min_similarity = similarity\n",
    "                dissimilar_idx = idx\n",
    "        return dissimilar_idx\n",
    "    \n",
    "# Get the Output Dimension of VGG16    \n",
    "def get_output_shape(model, image_dim, device):\n",
    "    model = model.to(device)  # Ensure the model is on the same device as the dummy input\n",
    "    with torch.no_grad():\n",
    "        dummy_input = torch.randn(1, 3, *image_dim).to(device)\n",
    "        output = model(dummy_input)\n",
    "    return output.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "dataPath = '../../Data/Processed_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrame\n",
    "furniture_dataset = load(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImgPath</th>\n",
       "      <th>FileType</th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "      <th>Ratio</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Bands</th>\n",
       "      <th>Transparency</th>\n",
       "      <th>Animated</th>\n",
       "      <th>Category</th>\n",
       "      <th>Interior_Style</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beds\\Asian\\19726asian-daybeds.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beds\\Asian\\20027asian-canopy-beds.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beds\\Asian\\20109asian-panel-beds.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beds\\Asian\\20508asian-platform-beds.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beds\\Asian\\20750asian-comforters-and-comforter...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90078</th>\n",
       "      <td>tables\\Victorian\\5victorian-side-tables-and-en...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90079</th>\n",
       "      <td>tables\\Victorian\\6victorian-side-tables-and-en...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90080</th>\n",
       "      <td>tables\\Victorian\\7victorian-side-tables-and-en...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90081</th>\n",
       "      <td>tables\\Victorian\\8victorian-dining-tables.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90082</th>\n",
       "      <td>tables\\Victorian\\9victorian-side-tables-and-en...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90083 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ImgPath FileType  Width  \\\n",
       "0                      beds\\Asian\\19726asian-daybeds.jpg      jpg    350   \n",
       "1                  beds\\Asian\\20027asian-canopy-beds.jpg      jpg    350   \n",
       "2                   beds\\Asian\\20109asian-panel-beds.jpg      jpg    350   \n",
       "3                beds\\Asian\\20508asian-platform-beds.jpg      jpg    350   \n",
       "4      beds\\Asian\\20750asian-comforters-and-comforter...      jpg    350   \n",
       "...                                                  ...      ...    ...   \n",
       "90078  tables\\Victorian\\5victorian-side-tables-and-en...      jpg    350   \n",
       "90079  tables\\Victorian\\6victorian-side-tables-and-en...      jpg    350   \n",
       "90080  tables\\Victorian\\7victorian-side-tables-and-en...      jpg    350   \n",
       "90081      tables\\Victorian\\8victorian-dining-tables.jpg      jpg    350   \n",
       "90082  tables\\Victorian\\9victorian-side-tables-and-en...      jpg    350   \n",
       "\n",
       "       Height  Ratio Mode  Bands  Transparency  Animated Category  \\\n",
       "0         350    1.0  RGB  R G B         False     False     beds   \n",
       "1         350    1.0  RGB  R G B         False     False     beds   \n",
       "2         350    1.0  RGB  R G B         False     False     beds   \n",
       "3         350    1.0  RGB  R G B         False     False     beds   \n",
       "4         350    1.0  RGB  R G B         False     False     beds   \n",
       "...       ...    ...  ...    ...           ...       ...      ...   \n",
       "90078     350    1.0  RGB  R G B         False     False   tables   \n",
       "90079     350    1.0  RGB  R G B         False     False   tables   \n",
       "90080     350    1.0  RGB  R G B         False     False   tables   \n",
       "90081     350    1.0  RGB  R G B         False     False   tables   \n",
       "90082     350    1.0  RGB  R G B         False     False   tables   \n",
       "\n",
       "      Interior_Style  \n",
       "0              Asian  \n",
       "1              Asian  \n",
       "2              Asian  \n",
       "3              Asian  \n",
       "4              Asian  \n",
       "...              ...  \n",
       "90078      Victorian  \n",
       "90079      Victorian  \n",
       "90080      Victorian  \n",
       "90081      Victorian  \n",
       "90082      Victorian  \n",
       "\n",
       "[90083 rows x 11 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the dataset\n",
    "furniture_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation logic\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((120, 120)),\n",
    "    torchvision.transforms.RandomCrop((110, 110)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VGG16 used for feature extraction\n",
    "class VGG16FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16FeatureExtractor, self).__init__()\n",
    "        \n",
    "        self.block_1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=3,\n",
    "                          out_channels=64,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=64,\n",
    "                          out_channels=64,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=64,\n",
    "                          out_channels=128,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=128,\n",
    "                          out_channels=128,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_3 = nn.Sequential(        \n",
    "                nn.Conv2d(in_channels=128,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),        \n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_4 = nn.Sequential(   \n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),        \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),        \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),            \n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_5 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),            \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),            \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),    \n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))             \n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.detach().zero_()\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.block_4(x)\n",
    "        x = self.block_5(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1) # flatten\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Siamese Network\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim=48):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.feature_extractor = VGG16FeatureExtractor()\n",
    "        out_features = get_output_shape(self.feature_extractor, (IMG_SIZE, IMG_SIZE), DEVICE)\n",
    "        self.feature_extractor.to(DEVICE)  # Move to device after getting output shape\n",
    "        self.fc = nn.Linear(out_features, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance layer\n",
    "class EuclideanDistance(nn.Module):\n",
    "    def forward(self, featsA, featsB):\n",
    "        return F.pairwise_distance(featsA, featsB, keepdim=True)\n",
    "\n",
    "# Define the complete Siamese Network model\n",
    "class SiameseModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=48):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.feature_extractor = SiameseNetwork(embedding_dim)\n",
    "        self.euclidean_distance = EuclideanDistance()\n",
    "        self.fc = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, inputA, inputB):\n",
    "        featsA = self.feature_extractor(inputA)\n",
    "        featsB = self.feature_extractor(inputB)\n",
    "        distance = self.euclidean_distance(featsA, featsB)\n",
    "        output = torch.sigmoid(self.fc(distance))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = SiameseModel().to(DEVICE)\n",
    "\n",
    "# Initialize weights\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       factor=0.1,\n",
    "                                                       mode='max',\n",
    "                                                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a chunk for training\n",
    "def process_chunk_train(chunk_df, epoch, chunk_idx, num_chunks):\n",
    "    pair_dataset = FurniturePairDataset(df=chunk_df, root_dir=dataPath, transform=transform)\n",
    "    pair_dataloader = DataLoader(pair_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    total_batches = len(pair_dataloader)\n",
    "    print(f\"Total samples in this chunk: {len(chunk_df)}\")\n",
    "    print(f\"Total batches in this chunk: {total_batches}\")\n",
    "    \n",
    "    for batch_idx, ((imageA, imageB), labels) in enumerate(pair_dataloader):\n",
    "        imageA, imageB, labels = imageA.cuda(), imageB.cuda(), labels.float().cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imageA, imageB)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Chunk {chunk_idx + 1}/{num_chunks}, Batch {batch_idx + 1}/{total_batches}, Loss: {running_loss / (batch_idx + 1)}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Chunk {chunk_idx + 1}/{num_chunks} completed in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    return running_loss / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a chunk for validation\n",
    "def process_chunk_validate(chunk_df):\n",
    "    pair_dataset = FurniturePairDataset(df=chunk_df, root_dir=dataPath, transform=transform)\n",
    "    pair_dataloader = DataLoader(pair_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_batches = len(pair_dataloader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, ((imageA, imageB), labels) in enumerate(pair_dataloader):\n",
    "            imageA, imageB, labels = imageA.cuda(), imageB.cuda(), labels.float().cuda()\n",
    "            outputs = model(imageA, imageB)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training chunk 1/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 1/2, Chunk 1/10, Batch 50/625, Loss: 0.1855951328575611\n",
      "Epoch 1/2, Chunk 1/10, Batch 100/625, Loss: 0.15455124989151955\n",
      "Epoch 1/2, Chunk 1/10, Batch 150/625, Loss: 0.13084308356046676\n",
      "Epoch 1/2, Chunk 1/10, Batch 200/625, Loss: 0.11534866256639362\n",
      "Epoch 1/2, Chunk 1/10, Batch 250/625, Loss: 0.10578932003676891\n",
      "Epoch 1/2, Chunk 1/10, Batch 300/625, Loss: 0.09693550775448481\n",
      "Epoch 1/2, Chunk 1/10, Batch 350/625, Loss: 0.09361679248511791\n",
      "Epoch 1/2, Chunk 1/10, Batch 400/625, Loss: 0.08969529538881034\n",
      "Epoch 1/2, Chunk 1/10, Batch 450/625, Loss: 0.08436372558275859\n",
      "Epoch 1/2, Chunk 1/10, Batch 500/625, Loss: 0.07948127645626664\n",
      "Epoch 1/2, Chunk 1/10, Batch 550/625, Loss: 0.07619765628637239\n",
      "Epoch 1/2, Chunk 1/10, Batch 600/625, Loss: 0.07322112262714654\n",
      "Epoch 1/2, Chunk 1/10 completed in 324.46 seconds\n",
      "Processing training chunk 2/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 1/2, Chunk 2/10, Batch 50/625, Loss: 0.1475439641624689\n",
      "Epoch 1/2, Chunk 2/10, Batch 100/625, Loss: 0.10803793389350176\n",
      "Epoch 1/2, Chunk 2/10, Batch 150/625, Loss: 0.09188167324910561\n",
      "Epoch 1/2, Chunk 2/10, Batch 200/625, Loss: 0.07643023148179054\n",
      "Epoch 1/2, Chunk 2/10, Batch 250/625, Loss: 0.06574479474686086\n",
      "Epoch 1/2, Chunk 2/10, Batch 300/625, Loss: 0.05797295753378421\n",
      "Epoch 1/2, Chunk 2/10, Batch 350/625, Loss: 0.05475679291811372\n",
      "Epoch 1/2, Chunk 2/10, Batch 400/625, Loss: 0.05216667428961955\n",
      "Epoch 1/2, Chunk 2/10, Batch 450/625, Loss: 0.04852887103230589\n",
      "Epoch 1/2, Chunk 2/10, Batch 500/625, Loss: 0.045457852001301945\n",
      "Epoch 1/2, Chunk 2/10, Batch 550/625, Loss: 0.042707708577032794\n",
      "Epoch 1/2, Chunk 2/10, Batch 600/625, Loss: 0.041081652026623484\n",
      "Epoch 1/2, Chunk 2/10 completed in 321.35 seconds\n",
      "Processing training chunk 3/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 1/2, Chunk 3/10, Batch 50/625, Loss: 0.21228892520070075\n",
      "Epoch 1/2, Chunk 3/10, Batch 100/625, Loss: 0.17316784440539779\n",
      "Epoch 1/2, Chunk 3/10, Batch 150/625, Loss: 0.1723820883470277\n",
      "Epoch 1/2, Chunk 3/10, Batch 200/625, Loss: 0.15147325018886476\n",
      "Epoch 1/2, Chunk 3/10, Batch 250/625, Loss: 0.15232458238676191\n",
      "Epoch 1/2, Chunk 3/10, Batch 300/625, Loss: 0.13488617786206306\n",
      "Epoch 1/2, Chunk 3/10, Batch 350/625, Loss: 0.11971169029761639\n",
      "Epoch 1/2, Chunk 3/10, Batch 400/625, Loss: 0.10966689153341576\n",
      "Epoch 1/2, Chunk 3/10, Batch 450/625, Loss: 0.10041010659188032\n",
      "Epoch 1/2, Chunk 3/10, Batch 500/625, Loss: 0.09278119241725653\n",
      "Epoch 1/2, Chunk 3/10, Batch 550/625, Loss: 0.08615671314976432\n",
      "Epoch 1/2, Chunk 3/10, Batch 600/625, Loss: 0.08072644075766827\n",
      "Epoch 1/2, Chunk 3/10 completed in 320.58 seconds\n",
      "Processing training chunk 4/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 1/2, Chunk 4/10, Batch 50/625, Loss: 0.2541464817523956\n",
      "Epoch 1/2, Chunk 4/10, Batch 100/625, Loss: 0.14679289558902384\n",
      "Epoch 1/2, Chunk 4/10, Batch 150/625, Loss: 0.10719590075314045\n",
      "Epoch 1/2, Chunk 4/10, Batch 200/625, Loss: 0.08626722530461847\n",
      "Epoch 1/2, Chunk 4/10, Batch 250/625, Loss: 0.07399978053942323\n",
      "Epoch 1/2, Chunk 4/10, Batch 300/625, Loss: 0.0652450733538717\n",
      "Epoch 1/2, Chunk 4/10, Batch 350/625, Loss: 0.058472192122467924\n",
      "Epoch 1/2, Chunk 4/10, Batch 400/625, Loss: 0.05282260130974464\n",
      "Epoch 1/2, Chunk 4/10, Batch 450/625, Loss: 0.04900839295341737\n",
      "Epoch 1/2, Chunk 4/10, Batch 500/625, Loss: 0.045160560845397414\n",
      "Epoch 1/2, Chunk 4/10, Batch 550/625, Loss: 0.04210318391803991\n",
      "Epoch 1/2, Chunk 4/10, Batch 600/625, Loss: 0.039750778269177924\n",
      "Epoch 1/2, Chunk 4/10 completed in 322.17 seconds\n",
      "Processing training chunk 5/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 1/2, Chunk 5/10, Batch 50/625, Loss: 0.02616720862220973\n",
      "Epoch 1/2, Chunk 5/10, Batch 100/625, Loss: 0.017946050746832042\n",
      "Epoch 1/2, Chunk 5/10, Batch 150/625, Loss: 0.02632481235700349\n",
      "Epoch 1/2, Chunk 5/10, Batch 200/625, Loss: 0.02375257298583165\n",
      "Epoch 1/2, Chunk 5/10, Batch 250/625, Loss: 0.02199766160082072\n",
      "Epoch 1/2, Chunk 5/10, Batch 300/625, Loss: 0.021368144517764448\n",
      "Epoch 1/2, Chunk 5/10, Batch 350/625, Loss: 0.02106002049986273\n",
      "Epoch 1/2, Chunk 5/10, Batch 400/625, Loss: 0.01976957711565774\n",
      "Epoch 1/2, Chunk 5/10, Batch 450/625, Loss: 0.018911842181761233\n",
      "Epoch 1/2, Chunk 5/10, Batch 500/625, Loss: 0.017615914372727276\n",
      "Epoch 1/2, Chunk 5/10, Batch 550/625, Loss: 0.017329027996059846\n",
      "Epoch 1/2, Chunk 5/10, Batch 600/625, Loss: 0.01704165290420254\n",
      "Epoch 1/2, Chunk 5/10 completed in 322.73 seconds\n",
      "Processing training chunk 6/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 1/2, Chunk 6/10, Batch 50/625, Loss: 0.042109334180131554\n",
      "Epoch 1/2, Chunk 6/10, Batch 100/625, Loss: 0.032709638373926285\n",
      "Epoch 1/2, Chunk 6/10, Batch 150/625, Loss: 0.026739036968598764\n",
      "Epoch 1/2, Chunk 6/10, Batch 200/625, Loss: 0.023086215438088403\n",
      "Epoch 1/2, Chunk 6/10, Batch 250/625, Loss: 0.020604951177723707\n",
      "Epoch 1/2, Chunk 6/10, Batch 300/625, Loss: 0.018902388340017447\n",
      "Epoch 1/2, Chunk 6/10, Batch 350/625, Loss: 0.017051375258847007\n",
      "Epoch 1/2, Chunk 6/10, Batch 400/625, Loss: 0.01616750869376119\n",
      "Epoch 1/2, Chunk 6/10, Batch 450/625, Loss: 0.015717069037879505\n",
      "Epoch 1/2, Chunk 6/10, Batch 500/625, Loss: 0.015251893795561045\n",
      "Epoch 1/2, Chunk 6/10, Batch 550/625, Loss: 0.014758176225322214\n",
      "Epoch 1/2, Chunk 6/10, Batch 600/625, Loss: 0.014032329961967965\n",
      "Epoch 1/2, Chunk 6/10 completed in 318.34 seconds\n",
      "Processing training chunk 7/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 1/2, Chunk 7/10, Batch 50/625, Loss: 0.21492150569334625\n",
      "Epoch 1/2, Chunk 7/10, Batch 100/625, Loss: 0.11456755050690845\n",
      "Epoch 1/2, Chunk 7/10, Batch 150/625, Loss: 0.08295995842820654\n",
      "Epoch 1/2, Chunk 7/10, Batch 200/625, Loss: 0.06831802606175189\n",
      "Epoch 1/2, Chunk 7/10, Batch 250/625, Loss: 0.05833477176213637\n",
      "Epoch 1/2, Chunk 7/10, Batch 300/625, Loss: 0.05388272067803579\n",
      "Epoch 1/2, Chunk 7/10, Batch 350/625, Loss: 0.04827356777153909\n",
      "Epoch 1/2, Chunk 7/10, Batch 400/625, Loss: 0.04302759680023883\n",
      "Epoch 1/2, Chunk 7/10, Batch 450/625, Loss: 0.03966739731426868\n",
      "Epoch 1/2, Chunk 7/10, Batch 500/625, Loss: 0.03745219649886712\n",
      "Epoch 1/2, Chunk 7/10, Batch 550/625, Loss: 0.035133317323316905\n",
      "Epoch 1/2, Chunk 7/10, Batch 600/625, Loss: 0.03300450021284632\n",
      "Epoch 1/2, Chunk 7/10 completed in 319.26 seconds\n",
      "Processing training chunk 8/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 1/2, Chunk 8/10, Batch 50/625, Loss: 0.11902745663188398\n",
      "Epoch 1/2, Chunk 8/10, Batch 100/625, Loss: 0.0701624593208544\n",
      "Epoch 1/2, Chunk 8/10, Batch 150/625, Loss: 0.05176702378317714\n",
      "Epoch 1/2, Chunk 8/10, Batch 200/625, Loss: 0.04156676624086685\n",
      "Epoch 1/2, Chunk 8/10, Batch 250/625, Loss: 0.0351423474997282\n",
      "Epoch 1/2, Chunk 8/10, Batch 300/625, Loss: 0.03071068138194581\n",
      "Epoch 1/2, Chunk 8/10, Batch 350/625, Loss: 0.028886053463710207\n",
      "Epoch 1/2, Chunk 8/10, Batch 400/625, Loss: 0.02634427609213162\n",
      "Epoch 1/2, Chunk 8/10, Batch 450/625, Loss: 0.024098548376415337\n",
      "Epoch 1/2, Chunk 8/10, Batch 500/625, Loss: 0.022535983779467642\n",
      "Epoch 1/2, Chunk 8/10, Batch 550/625, Loss: 0.02164105322973972\n",
      "Epoch 1/2, Chunk 8/10, Batch 600/625, Loss: 0.02042165211789931\n",
      "Epoch 1/2, Chunk 8/10 completed in 322.00 seconds\n",
      "Processing training chunk 9/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 1/2, Chunk 9/10, Batch 50/625, Loss: 0.05752057351171971\n",
      "Epoch 1/2, Chunk 9/10, Batch 100/625, Loss: 0.056248969258740546\n",
      "Epoch 1/2, Chunk 9/10, Batch 150/625, Loss: 0.04742275403346866\n",
      "Epoch 1/2, Chunk 9/10, Batch 200/625, Loss: 0.04054163477034308\n",
      "Epoch 1/2, Chunk 9/10, Batch 250/625, Loss: 0.038393400251865384\n",
      "Epoch 1/2, Chunk 9/10, Batch 300/625, Loss: 0.034416967265618346\n",
      "Epoch 1/2, Chunk 9/10, Batch 350/625, Loss: 0.03176601599436253\n",
      "Epoch 1/2, Chunk 9/10, Batch 400/625, Loss: 0.029974468688596972\n",
      "Epoch 1/2, Chunk 9/10, Batch 450/625, Loss: 0.029879075840322507\n",
      "Epoch 1/2, Chunk 9/10, Batch 500/625, Loss: 0.028848415827844292\n",
      "Epoch 1/2, Chunk 9/10, Batch 550/625, Loss: 0.029484768751585348\n",
      "Epoch 1/2, Chunk 9/10, Batch 600/625, Loss: 0.02887595029703031\n",
      "Epoch 1/2, Chunk 9/10 completed in 323.53 seconds\n",
      "Processing training chunk 10/10\n",
      "Total samples in this chunk: 83\n",
      "Total batches in this chunk: 6\n",
      "Epoch 1/2, Chunk 10/10 completed in 1.17 seconds\n",
      "Epoch 1/2, Training Loss: 0.06945604742251958, Validation Loss: 0.141560141569376\n",
      "Processing training chunk 1/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 2/2, Chunk 1/10, Batch 50/625, Loss: 0.04296834306791425\n",
      "Epoch 2/2, Chunk 1/10, Batch 100/625, Loss: 0.031370607051067055\n",
      "Epoch 2/2, Chunk 1/10, Batch 150/625, Loss: 0.0278957511143138\n",
      "Epoch 2/2, Chunk 1/10, Batch 200/625, Loss: 0.024769927267916502\n",
      "Epoch 2/2, Chunk 1/10, Batch 250/625, Loss: 0.0212968481881544\n",
      "Epoch 2/2, Chunk 1/10, Batch 300/625, Loss: 0.019522326375590638\n",
      "Epoch 2/2, Chunk 1/10, Batch 350/625, Loss: 0.01752552355672898\n",
      "Epoch 2/2, Chunk 1/10, Batch 400/625, Loss: 0.018457703187305014\n",
      "Epoch 2/2, Chunk 1/10, Batch 450/625, Loss: 0.017778767465044643\n",
      "Epoch 2/2, Chunk 1/10, Batch 500/625, Loss: 0.017708950510481374\n",
      "Epoch 2/2, Chunk 1/10, Batch 550/625, Loss: 0.01796541945479641\n",
      "Epoch 2/2, Chunk 1/10, Batch 600/625, Loss: 0.017100783946807495\n",
      "Epoch 2/2, Chunk 1/10 completed in 329.67 seconds\n",
      "Processing training chunk 2/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 2/2, Chunk 2/10, Batch 50/625, Loss: 0.13946970170829445\n",
      "Epoch 2/2, Chunk 2/10, Batch 100/625, Loss: 0.0759420408133883\n",
      "Epoch 2/2, Chunk 2/10, Batch 150/625, Loss: 0.05880989268111686\n",
      "Epoch 2/2, Chunk 2/10, Batch 200/625, Loss: 0.04772089708596468\n",
      "Epoch 2/2, Chunk 2/10, Batch 250/625, Loss: 0.039938493749126794\n",
      "Epoch 2/2, Chunk 2/10, Batch 300/625, Loss: 0.03629104579760072\n",
      "Epoch 2/2, Chunk 2/10, Batch 350/625, Loss: 0.0322728752175213\n",
      "Epoch 2/2, Chunk 2/10, Batch 400/625, Loss: 0.028976810915628447\n",
      "Epoch 2/2, Chunk 2/10, Batch 450/625, Loss: 0.02652863668019159\n",
      "Epoch 2/2, Chunk 2/10, Batch 500/625, Loss: 0.02446671760897152\n",
      "Epoch 2/2, Chunk 2/10, Batch 550/625, Loss: 0.023035059947181833\n",
      "Epoch 2/2, Chunk 2/10, Batch 600/625, Loss: 0.022048791149475923\n",
      "Epoch 2/2, Chunk 2/10 completed in 322.14 seconds\n",
      "Processing training chunk 3/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 2/2, Chunk 3/10, Batch 50/625, Loss: 0.1821372901648283\n",
      "Epoch 2/2, Chunk 3/10, Batch 100/625, Loss: 0.12505723397247492\n",
      "Epoch 2/2, Chunk 3/10, Batch 150/625, Loss: 0.10118351627296458\n",
      "Epoch 2/2, Chunk 3/10, Batch 200/625, Loss: 0.08120321957510895\n",
      "Epoch 2/2, Chunk 3/10, Batch 250/625, Loss: 0.06812023483589291\n",
      "Epoch 2/2, Chunk 3/10, Batch 300/625, Loss: 0.06299094579027345\n",
      "Epoch 2/2, Chunk 3/10, Batch 350/625, Loss: 0.055998958852807326\n",
      "Epoch 2/2, Chunk 3/10, Batch 400/625, Loss: 0.0507404118849081\n",
      "Epoch 2/2, Chunk 3/10, Batch 450/625, Loss: 0.04607240655976865\n",
      "Epoch 2/2, Chunk 3/10, Batch 500/625, Loss: 0.04362948974687606\n",
      "Epoch 2/2, Chunk 3/10, Batch 550/625, Loss: 0.040345968643457374\n",
      "Epoch 2/2, Chunk 3/10, Batch 600/625, Loss: 0.0378318328394865\n",
      "Epoch 2/2, Chunk 3/10 completed in 321.75 seconds\n",
      "Processing training chunk 4/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 2/2, Chunk 4/10, Batch 50/625, Loss: 0.21612765604630113\n",
      "Epoch 2/2, Chunk 4/10, Batch 100/625, Loss: 0.11718799092806875\n",
      "Epoch 2/2, Chunk 4/10, Batch 150/625, Loss: 0.08516708655127635\n",
      "Epoch 2/2, Chunk 4/10, Batch 200/625, Loss: 0.0726432045653928\n",
      "Epoch 2/2, Chunk 4/10, Batch 250/625, Loss: 0.06225506840646267\n",
      "Epoch 2/2, Chunk 4/10, Batch 300/625, Loss: 0.053788220383382095\n",
      "Epoch 2/2, Chunk 4/10, Batch 350/625, Loss: 0.04763758285104164\n",
      "Epoch 2/2, Chunk 4/10, Batch 400/625, Loss: 0.04315082529210486\n",
      "Epoch 2/2, Chunk 4/10, Batch 450/625, Loss: 0.0390736579343987\n",
      "Epoch 2/2, Chunk 4/10, Batch 500/625, Loss: 0.035787838496733455\n",
      "Epoch 2/2, Chunk 4/10, Batch 550/625, Loss: 0.03331927439367229\n",
      "Epoch 2/2, Chunk 4/10, Batch 600/625, Loss: 0.03152507624491894\n",
      "Epoch 2/2, Chunk 4/10 completed in 315.71 seconds\n",
      "Processing training chunk 5/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 2/2, Chunk 5/10, Batch 50/625, Loss: 0.03801617550430819\n",
      "Epoch 2/2, Chunk 5/10, Batch 100/625, Loss: 0.03051186451513786\n",
      "Epoch 2/2, Chunk 5/10, Batch 150/625, Loss: 0.02571062688676951\n",
      "Epoch 2/2, Chunk 5/10, Batch 200/625, Loss: 0.02221669397171354\n",
      "Epoch 2/2, Chunk 5/10, Batch 250/625, Loss: 0.01968431295850314\n",
      "Epoch 2/2, Chunk 5/10, Batch 300/625, Loss: 0.018907484565279447\n",
      "Epoch 2/2, Chunk 5/10, Batch 350/625, Loss: 0.018109250250272452\n",
      "Epoch 2/2, Chunk 5/10, Batch 400/625, Loss: 0.016335632164264098\n",
      "Epoch 2/2, Chunk 5/10, Batch 450/625, Loss: 0.015201583535203502\n",
      "Epoch 2/2, Chunk 5/10, Batch 500/625, Loss: 0.014506162392906845\n",
      "Epoch 2/2, Chunk 5/10, Batch 550/625, Loss: 0.013698532936819406\n",
      "Epoch 2/2, Chunk 5/10, Batch 600/625, Loss: 0.013158895830953648\n",
      "Epoch 2/2, Chunk 5/10 completed in 316.68 seconds\n",
      "Processing training chunk 6/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 2/2, Chunk 6/10, Batch 50/625, Loss: 0.08650746572529897\n",
      "Epoch 2/2, Chunk 6/10, Batch 100/625, Loss: 0.04877070186077617\n",
      "Epoch 2/2, Chunk 6/10, Batch 150/625, Loss: 0.0364290784796079\n",
      "Epoch 2/2, Chunk 6/10, Batch 200/625, Loss: 0.03191907656058902\n",
      "Epoch 2/2, Chunk 6/10, Batch 250/625, Loss: 0.02640399869182147\n",
      "Epoch 2/2, Chunk 6/10, Batch 300/625, Loss: 0.023192734355689026\n",
      "Epoch 2/2, Chunk 6/10, Batch 350/625, Loss: 0.021079490438735644\n",
      "Epoch 2/2, Chunk 6/10, Batch 400/625, Loss: 0.01941404443598003\n",
      "Epoch 2/2, Chunk 6/10, Batch 450/625, Loss: 0.018887376666146642\n",
      "Epoch 2/2, Chunk 6/10, Batch 500/625, Loss: 0.017719222732121124\n",
      "Epoch 2/2, Chunk 6/10, Batch 550/625, Loss: 0.016692231584932993\n",
      "Epoch 2/2, Chunk 6/10, Batch 600/625, Loss: 0.01569905247364659\n",
      "Epoch 2/2, Chunk 6/10 completed in 316.56 seconds\n",
      "Processing training chunk 7/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 2/2, Chunk 7/10, Batch 50/625, Loss: 0.11117977878078818\n",
      "Epoch 2/2, Chunk 7/10, Batch 100/625, Loss: 0.0677806369937025\n",
      "Epoch 2/2, Chunk 7/10, Batch 150/625, Loss: 0.04699442793809188\n",
      "Epoch 2/2, Chunk 7/10, Batch 200/625, Loss: 0.036797731768456285\n",
      "Epoch 2/2, Chunk 7/10, Batch 250/625, Loss: 0.03025767371430993\n",
      "Epoch 2/2, Chunk 7/10, Batch 300/625, Loss: 0.028142791559997327\n",
      "Epoch 2/2, Chunk 7/10, Batch 350/625, Loss: 0.02479280917838748\n",
      "Epoch 2/2, Chunk 7/10, Batch 400/625, Loss: 0.022258492785476848\n",
      "Epoch 2/2, Chunk 7/10, Batch 450/625, Loss: 0.02031515187208748\n",
      "Epoch 2/2, Chunk 7/10, Batch 500/625, Loss: 0.019016593040316367\n",
      "Epoch 2/2, Chunk 7/10, Batch 550/625, Loss: 0.018825297504980965\n",
      "Epoch 2/2, Chunk 7/10, Batch 600/625, Loss: 0.019775902995412858\n",
      "Epoch 2/2, Chunk 7/10 completed in 315.33 seconds\n",
      "Processing training chunk 8/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 2/2, Chunk 8/10, Batch 50/625, Loss: 0.09185854207491502\n",
      "Epoch 2/2, Chunk 8/10, Batch 100/625, Loss: 0.05508830960490741\n",
      "Epoch 2/2, Chunk 8/10, Batch 150/625, Loss: 0.0409974220697768\n",
      "Epoch 2/2, Chunk 8/10, Batch 200/625, Loss: 0.032583594568423\n",
      "Epoch 2/2, Chunk 8/10, Batch 250/625, Loss: 0.029425540152704342\n",
      "Epoch 2/2, Chunk 8/10, Batch 300/625, Loss: 0.026309997385251336\n",
      "Epoch 2/2, Chunk 8/10, Batch 350/625, Loss: 0.02436565440380946\n",
      "Epoch 2/2, Chunk 8/10, Batch 400/625, Loss: 0.021941083527635782\n",
      "Epoch 2/2, Chunk 8/10, Batch 450/625, Loss: 0.020100052481672417\n",
      "Epoch 2/2, Chunk 8/10, Batch 500/625, Loss: 0.018609643956413492\n",
      "Epoch 2/2, Chunk 8/10, Batch 550/625, Loss: 0.01859509540129114\n",
      "Epoch 2/2, Chunk 8/10, Batch 600/625, Loss: 0.017896063918985117\n",
      "Epoch 2/2, Chunk 8/10 completed in 317.76 seconds\n",
      "Processing training chunk 9/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 625\n",
      "Epoch 2/2, Chunk 9/10, Batch 50/625, Loss: 0.13370376923354343\n",
      "Epoch 2/2, Chunk 9/10, Batch 100/625, Loss: 0.08186586377560161\n",
      "Epoch 2/2, Chunk 9/10, Batch 150/625, Loss: 0.059457083931192756\n",
      "Epoch 2/2, Chunk 9/10, Batch 200/625, Loss: 0.04730919400637504\n",
      "Epoch 2/2, Chunk 9/10, Batch 250/625, Loss: 0.03981554411398247\n",
      "Epoch 2/2, Chunk 9/10, Batch 300/625, Loss: 0.034814864255022254\n",
      "Epoch 2/2, Chunk 9/10, Batch 350/625, Loss: 0.032343346646188625\n",
      "Epoch 2/2, Chunk 9/10, Batch 400/625, Loss: 0.029230143825989218\n",
      "Epoch 2/2, Chunk 9/10, Batch 450/625, Loss: 0.027629303839187033\n",
      "Epoch 2/2, Chunk 9/10, Batch 500/625, Loss: 0.02604715488420334\n",
      "Epoch 2/2, Chunk 9/10, Batch 550/625, Loss: 0.024817915957061235\n",
      "Epoch 2/2, Chunk 9/10, Batch 600/625, Loss: 0.0236284921055388\n",
      "Epoch 2/2, Chunk 9/10 completed in 316.25 seconds\n",
      "Processing training chunk 10/10\n",
      "Total samples in this chunk: 83\n",
      "Total batches in this chunk: 6\n",
      "Epoch 2/2, Chunk 10/10 completed in 1.00 seconds\n",
      "Epoch 2/2, Training Loss: 0.052483955971254034, Validation Loss: 0.05438995656631887\n",
      "Training Siamese Baseline Model time: 7245.77 seconds\n"
     ]
    }
   ],
   "source": [
    "# Process the dataset in chunks for training\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    num_chunks = len(furniture_dataset) // CHUNK_SIZE + 1\n",
    "    total_train_loss = 0.0\n",
    "    \n",
    "    for chunk_idx in range(num_chunks):\n",
    "        chunk_df = furniture_dataset.iloc[chunk_idx * CHUNK_SIZE:(chunk_idx + 1) * CHUNK_SIZE]\n",
    "        print(f\"Processing training chunk {chunk_idx + 1}/{num_chunks}\")\n",
    "        train_loss = process_chunk_train(chunk_df, epoch, chunk_idx, num_chunks)\n",
    "        total_train_loss += train_loss\n",
    "    \n",
    "    avg_train_loss = total_train_loss / num_chunks\n",
    "    \n",
    "    # Perform validation on a chunk of the dataset\n",
    "    validation_chunk_df = furniture_dataset.iloc[:CHUNK_SIZE]  # Use the first chunk for validation\n",
    "    validation_loss = process_chunk_validate(validation_chunk_df)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Training Loss: {avg_train_loss}, Validation Loss: {validation_loss}\")\n",
    "    scheduler.step(validation_loss)  # Update the learning rate scheduler with the validation loss\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training Siamese Baseline Model time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and optimizer state saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the model and optimizer state\n",
    "torch.save(model.state_dict(), 'Siamese_(VGG16).pth')\n",
    "torch.save(optimizer.state_dict(), 'Optimizer_(with_VGG16).pth')\n",
    "\n",
    "print(\"Model and optimizer state saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation pipeline saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the transformation pipeline\n",
    "with open('Transform.pkl', 'wb') as f:\n",
    "    pickle.dump(transform, f)\n",
    "\n",
    "print(\"Transformation pipeline saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
