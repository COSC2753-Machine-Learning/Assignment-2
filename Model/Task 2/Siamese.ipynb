{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import os, sys\n",
    "import torch\n",
    "import torchvision\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Dropout, GlobalAveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local helper files\n",
    "from utils.dataset import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CUDA devices: 1\n",
      "Device 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available CUDA devices: {num_gpus}\")\n",
    "    \n",
    "    # Print the name of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the shape of the inputs for our network\n",
    "IMG_SIZE = 120\n",
    "CROP_SIZE = 110\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the batch size and number of epochs\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 8\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predefined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined functions\n",
    "# Function to load the dataset into Siamese Model\n",
    "class FashionPairDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        self.data_df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.color_histograms = []\n",
    "        self.edge_histograms = []\n",
    "        self._load_images_and_features()\n",
    "\n",
    "    def _load_images_and_features(self):\n",
    "        for idx in range(len(self.data_df)):\n",
    "            img_name = os.path.join(self.root_dir, self.data_df.iloc[idx, 0])\n",
    "            image = Image.open(img_name)\n",
    "            self.images.append(image)\n",
    "            self.color_histograms.append(self._extract_color_histogram(image))\n",
    "            self.edge_histograms.append(self._extract_edge_histogram(image))\n",
    "\n",
    "    def _extract_color_histogram(self, image):\n",
    "        image = np.array(image.convert(\"RGB\"))\n",
    "        hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "        cv2.normalize(hist, hist)\n",
    "        return hist.flatten()\n",
    "\n",
    "    def _extract_edge_histogram(self, image):\n",
    "        image = np.array(image.convert(\"L\"))\n",
    "        edges = cv2.Canny(image, 100, 200)\n",
    "        hist = cv2.calcHist([edges], [0], None, [256], [0, 256])\n",
    "        cv2.normalize(hist, hist)\n",
    "        return hist.flatten()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_df) * 2  # Each image will appear in one positive and one negative pair\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the anchor image and its label\n",
    "        anchor_idx = idx // 2\n",
    "        anchor_image = self.images[anchor_idx]\n",
    "        anchor_color_hist = self.color_histograms[anchor_idx]\n",
    "        anchor_edge_hist = self.edge_histograms[anchor_idx]\n",
    "\n",
    "        if self.transform:\n",
    "            anchor_image = self.transform(anchor_image)\n",
    "        \n",
    "        if idx % 2 == 0:\n",
    "            # Positive pair based on color similarity\n",
    "            similar_idx = self._find_most_similar(anchor_idx, anchor_color_hist, self.color_histograms)\n",
    "            similar_image = self.images[similar_idx]\n",
    "            \n",
    "            if self.transform:\n",
    "                similar_image = self.transform(similar_image)\n",
    "                \n",
    "            return (anchor_image, similar_image), 1\n",
    "        else:\n",
    "            # Negative pair based on edge dissimilarity\n",
    "            dissimilar_idx = self._find_most_dissimilar(anchor_idx, anchor_edge_hist, self.edge_histograms)\n",
    "            dissimilar_image = self.images[dissimilar_idx]\n",
    "            \n",
    "            if self.transform:\n",
    "                dissimilar_image = self.transform(dissimilar_image)\n",
    "                \n",
    "            return (anchor_image, dissimilar_image), 0\n",
    "    \n",
    "    def _find_most_similar(self, anchor_idx, anchor_hist, histograms):\n",
    "        similarities = [cv2.compareHist(anchor_hist, hist, cv2.HISTCMP_CORREL) for hist in histograms]\n",
    "        similarities[anchor_idx] = -1  # Ignore the anchor image itself\n",
    "        return np.argmax(similarities)\n",
    "\n",
    "    def _find_most_dissimilar(self, anchor_idx, anchor_hist, histograms):\n",
    "        dissimilarities = [cv2.compareHist(anchor_hist, hist, cv2.HISTCMP_CORREL) for hist in histograms]\n",
    "        dissimilarities[anchor_idx] = 1  # Ignore the anchor image itself\n",
    "        return np.argmin(dissimilarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "rawDataPath = '../../Data/Furniture_Data/'\n",
    "train_val_test_split = [0.8, 0.15, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing ../../Data/Furniture_Data/lamps\\Modern\\11286modern-lighting.jpg: [Errno 13] Permission denied: '../../Data/Furniture_Data/lamps\\\\Modern\\\\11286modern-lighting.jpg'\n"
     ]
    }
   ],
   "source": [
    "# Load DataFrame\n",
    "furniture_dataset = load(rawDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImgPath</th>\n",
       "      <th>FileType</th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "      <th>Ratio</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Bands</th>\n",
       "      <th>Transparency</th>\n",
       "      <th>Animated</th>\n",
       "      <th>Category</th>\n",
       "      <th>Interior_Style</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beds\\Asian\\19726asian-daybeds.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beds\\Asian\\20027asian-canopy-beds.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beds\\Asian\\20109asian-panel-beds.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beds\\Asian\\20508asian-platform-beds.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beds\\Asian\\20750asian-comforters-and-comforter...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90078</th>\n",
       "      <td>tables\\Victorian\\5victorian-side-tables-and-en...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90079</th>\n",
       "      <td>tables\\Victorian\\6victorian-side-tables-and-en...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90080</th>\n",
       "      <td>tables\\Victorian\\7victorian-side-tables-and-en...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90081</th>\n",
       "      <td>tables\\Victorian\\8victorian-dining-tables.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90082</th>\n",
       "      <td>tables\\Victorian\\9victorian-side-tables-and-en...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90083 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ImgPath FileType  Width  \\\n",
       "0                      beds\\Asian\\19726asian-daybeds.jpg      jpg    350   \n",
       "1                  beds\\Asian\\20027asian-canopy-beds.jpg      jpg    350   \n",
       "2                   beds\\Asian\\20109asian-panel-beds.jpg      jpg    350   \n",
       "3                beds\\Asian\\20508asian-platform-beds.jpg      jpg    350   \n",
       "4      beds\\Asian\\20750asian-comforters-and-comforter...      jpg    350   \n",
       "...                                                  ...      ...    ...   \n",
       "90078  tables\\Victorian\\5victorian-side-tables-and-en...      jpg    350   \n",
       "90079  tables\\Victorian\\6victorian-side-tables-and-en...      jpg    350   \n",
       "90080  tables\\Victorian\\7victorian-side-tables-and-en...      jpg    350   \n",
       "90081      tables\\Victorian\\8victorian-dining-tables.jpg      jpg    350   \n",
       "90082  tables\\Victorian\\9victorian-side-tables-and-en...      jpg    350   \n",
       "\n",
       "       Height  Ratio Mode  Bands  Transparency  Animated Category  \\\n",
       "0         350    1.0  RGB  R G B         False     False     beds   \n",
       "1         350    1.0  RGB  R G B         False     False     beds   \n",
       "2         350    1.0  RGB  R G B         False     False     beds   \n",
       "3         350    1.0  RGB  R G B         False     False     beds   \n",
       "4         350    1.0  RGB  R G B         False     False     beds   \n",
       "...       ...    ...  ...    ...           ...       ...      ...   \n",
       "90078     350    1.0  RGB  R G B         False     False   tables   \n",
       "90079     350    1.0  RGB  R G B         False     False   tables   \n",
       "90080     350    1.0  RGB  R G B         False     False   tables   \n",
       "90081     350    1.0  RGB  R G B         False     False   tables   \n",
       "90082     350    1.0  RGB  R G B         False     False   tables   \n",
       "\n",
       "      Interior_Style  \n",
       "0              Asian  \n",
       "1              Asian  \n",
       "2              Asian  \n",
       "3              Asian  \n",
       "4              Asian  \n",
       "...              ...  \n",
       "90078      Victorian  \n",
       "90079      Victorian  \n",
       "90080      Victorian  \n",
       "90081      Victorian  \n",
       "90082      Victorian  \n",
       "\n",
       "[90083 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "furniture_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((120, 120)),\n",
    "    torchvision.transforms.RandomCrop((110, 110)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Create the pair dataset\n",
    "pair_dataset = FashionPairDataset(df=furniture_dataset, \n",
    "                                  root_dir=rawDataPath, \n",
    "                                  transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_dataloader = DataLoader(pair_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 110, 110]) torch.Size([256, 3, 110, 110]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# Check one batch\n",
    "for (imageA, imageB), label in pair_dataloader:\n",
    "    print(imageA.shape, imageB.shape, label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Siamese Network using PyTorch\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, embedding_dim=48):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=2, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=2, padding=1)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(64, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(-1, 64)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Euclidean distance layer\n",
    "class EuclideanDistance(nn.Module):\n",
    "    def forward(self, featsA, featsB):\n",
    "        return F.pairwise_distance(featsA, featsB)\n",
    "\n",
    "# Define the complete Siamese Network model\n",
    "class SiameseModel(nn.Module):\n",
    "    def __init__(self, input_shape, embedding_dim=48):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.feature_extractor = SiameseNetwork(input_shape, embedding_dim)\n",
    "        self.euclidean_distance = EuclideanDistance()\n",
    "        self.fc = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, inputA, inputB):\n",
    "        featsA = self.feature_extractor(inputA)\n",
    "        featsB = self.feature_extractor(inputB)\n",
    "        distance = self.euclidean_distance(featsA, featsB)\n",
    "        output = torch.sigmoid(self.fc(distance))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = SiameseModel(IMG_SHAPE).to(DEVICE)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       factor=0.1,\n",
    "                                                       mode='max',\n",
    "                                                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_batches = len(pair_dataloader)\n",
    "    for batch_idx, ((imageA, imageB), labels) in enumerate(pair_dataloader):\n",
    "        imageA, imageB, labels = imageA.cuda(), imageB.cuda(), labels.float().cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imageA, imageB)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
