{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import os, sys\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import IPython\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the shape of the inputs for our network\n",
    "IMG_SIZE = 224\n",
    "CROP_SIZE = 110\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the batch size and number of epochs\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 8\n",
    "EPOCHS_MLP = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the base output directory\n",
    "BASE_OUTPUT = \"../../Results\"\n",
    "# Use the base output path to derive the path to the serialized\n",
    "# Model along with training history plot\n",
    "MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"siamese_model\"])\n",
    "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined functions\n",
    "\n",
    "def contrastive_loss(y, preds, margin=1):\n",
    "\t# Explicitly cast the true class label data type to the predicted class label data type (otherwise we run the risk of having two separate data types, causing TensorFlow to error out)\n",
    "\ty = tf.cast(y, preds.dtype)\n",
    "\t\n",
    "    # Calculate the contrastive loss between the true labels and the predicted labels\n",
    "\tsquaredPreds = K.square(preds)\n",
    "\tsquaredMargin = K.square(K.maximum(margin - preds, 0))\n",
    "\tloss = K.mean((1 - y) * squaredPreds + y * squaredMargin)\n",
    "\t\n",
    "\t# Return the computed contrastive loss to the calling function\n",
    "\treturn loss\n",
    "\n",
    "def build_siamese_model(inputShape, embeddingDim=48):\n",
    "\t# Specify the inputs for the feature extractor network\n",
    "\tinputs = Input(inputShape)\n",
    " \n",
    "\t# Define the first set of CONV => RELU => POOL => DROPOUT layers 64 filters 2x2\n",
    "\tx = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(inputs) # Si entrada 28x28x1 -> 28x28x64\n",
    "\tx = MaxPooling2D(pool_size=2)(x) # Si entrada 28x28x64 -> 14x14x64\n",
    "\tx = Dropout(0.3)(x)\n",
    " \n",
    "\t# Second set of CONV => RELU => POOL => DROPOUT layers 64 filters 2x2\n",
    "\tx = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(x) # Si entrada 14x14x64 -> 14x14x64\n",
    "\tx = MaxPooling2D(pool_size=2)(x) # Si entrada 14x14x64 -> 7x7x64\n",
    "\tx = Dropout(0.3)(x)\n",
    " \n",
    " \t# Prepare the final outputs\n",
    "\tpooledOutput = GlobalAveragePooling2D()(x)\n",
    "\toutputs = Dense(embeddingDim)(pooledOutput)\n",
    "\t\n",
    "\t# Build the model\n",
    "\tmodel = Model(inputs, outputs)\n",
    "\t\n",
    "\t# Return the model to the calling function\n",
    "\treturn model\n",
    "\n",
    "def get_pairs_route(images_dir, data_df, image_shape):\n",
    "    images = []\n",
    "    labels = []\n",
    "    names = []\n",
    "    # We use the product type as different classes\n",
    "    numClasses = data_df[\"Category\"].unique()\n",
    "\n",
    "    for category in os.listdir(images_dir):\n",
    "        category_path = os.path.join(images_dir, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            for style in os.listdir(category_path):\n",
    "                style_path = os.path.join(category_path, style)\n",
    "                if os.path.isdir(style_path):\n",
    "                    for img_name in os.listdir(style_path):\n",
    "                        img_path = os.path.join(style_path, img_name)\n",
    "                        img_name = os.path.join(category, style, img_name)\n",
    "                        try:\n",
    "                            matches = data_df[data_df[\"ImgPath\"] == img_name]\n",
    "                            if len(matches) != 1:\n",
    "                                raise ValueError(f\"Expected one match for {img_name}, but found {len(matches)}\")\n",
    "\n",
    "                            label = matches.Category.item()  # Convert the single value to a scalar\n",
    "                            # Load the image\n",
    "                            image = Image.open(img_path)\n",
    "                            \n",
    "                            # All images should be the same size\n",
    "                            image_resize = image.resize((image_shape[0], image_shape[1]))\n",
    "\n",
    "                            # Initialize channel_img to None\n",
    "                            channel_img = None\n",
    "                            \n",
    "                            # Convert image to numpy array\n",
    "                            if image_shape[2] == 1:\n",
    "                                channel_img = image_resize.convert(\"L\")\n",
    "                            elif image_shape[2] == 3:\n",
    "                                channel_img = image_resize.convert(\"RGB\")\n",
    "                            \n",
    "                            if channel_img is None:\n",
    "                                raise ValueError(f\"Unexpected number of channels in image: {img_path}\")\n",
    "            \n",
    "                            data = np.asarray(channel_img)\n",
    "                            \n",
    "                            images.append(data)\n",
    "                            labels.append(label)\n",
    "                            names.append(img_name)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "    return np.stack(images), np.stack(labels), np.stack(names)\n",
    "\n",
    "def make_pairs(images, labels):\n",
    "\t# Initialize two empty lists to hold the (image, image) pairs and labels to indicate if a pair is positive or negative\n",
    "\tpairImages = []\n",
    "\tpairLabels = []\n",
    "\t\n",
    "\t# Calculate the total number of classes present in the dataset and then build a list of indexes for each class label that provides the indexes for all examples with a given label\n",
    "\n",
    "\tclasses = np.unique(labels)\n",
    " \n",
    "\tidx = [np.where(labels == i)[0] for i in classes]\n",
    "\t\n",
    "\t# Loop over all images\n",
    "\tfor idxA in range(len(images)):\n",
    "\t\t\n",
    "\t\t# grab the current image and label belonging to the current iteration\n",
    "\t\tcurrentImage = images[idxA]\n",
    "\t\tlabel = labels[idxA]\n",
    "\t\tlabel_idx = np.where(classes == label)[0][0]\n",
    "\t\t\n",
    "\t\t# Randomly pick an image that belongs to the *same* class label\n",
    "\t\tidxB = np.random.choice(idx[label_idx])\n",
    "\t\tposImage = images[idxB]\n",
    "\t\t\n",
    "\t\t# Prepare a positive pair and update the images and labels lists, respectively\n",
    "\t\tpairImages.append([currentImage, posImage])\n",
    "\t\tpairLabels.append([1])\n",
    "\t\t\n",
    "\t\t# Grab the indices for each of the class labels *not* equal to the current label and randomly pick an image corresponding to a label *not* equal to the current label\n",
    "\t\tnegIdx = np.where(labels != label)[0]\n",
    "\t\tnegImage = images[np.random.choice(negIdx)]\n",
    "\t\t\n",
    "\t\t# Prepare a negative pair of images and update our lists\n",
    "\t\tpairImages.append([currentImage, negImage])\n",
    "\t\tpairLabels.append([0])\n",
    "\t\t\n",
    "\t# Return a 2-tuple of our image pairs and labels\n",
    "\treturn (np.array(pairImages), np.array(pairLabels))\n",
    "\n",
    "\n",
    "\n",
    "def euclidean_distance(vectors):\n",
    "\t# Unpack the vectors into separate lists\n",
    "\t(featsA, featsB) = vectors\n",
    "\n",
    "\t# Compute the sum of squared distances between the vectors\n",
    "\tsumSquared = K.sum(K.square(featsA - featsB), axis=1, keepdims=True)\n",
    "    \n",
    "\t# Return the euclidean distance between the vectors\n",
    "\treturn K.sqrt(K.maximum(sumSquared, K.epsilon()))\n",
    "     \n",
    "\n",
    "def plot_training(H, plotPath):\n",
    "\t# Construct a plot that plots and saves the training history\n",
    "\tplt.style.use(\"ggplot\")\n",
    "\tplt.figure()\n",
    "\tplt.plot(H.history[\"loss\"], label=\"train_loss\")\n",
    "\tplt.plot(H.history[\"val_loss\"], label=\"val_loss\")\n",
    "\tplt.plot(H.history[\"f1_m\"], label=\"train_f1\")\n",
    "\tplt.plot(H.history[\"val_f1_m\"], label=\"val_f1\")\n",
    "\tplt.title(\"Training Loss and Accuracy\")\n",
    "\tplt.xlabel(\"Epoch #\")\n",
    "\tplt.ylabel(\"Loss/F1\")\n",
    "\tplt.legend(loc=\"lower left\")\n",
    "\tplt.savefig(plotPath)\n",
    "     \n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "     \n",
    "\n",
    "def visualize_test(pairTrain_visualize, model_visualize, size=10, seed=42):\n",
    "  np.random.seed(seed)\n",
    "  sample_index = np.random.randint(pairTrain_visualize.shape[0], size=size)\n",
    "\n",
    "  # Loop over all image pairs\n",
    "  for (i, (imageA, imageB)) in enumerate(pairTrain_visualize[sample_index]):\n",
    "    \n",
    "    # Load both the images and convert them to grayscale create a copy of both the images for visualization purpose\n",
    "    origA = imageA.copy()\n",
    "    origB = imageB.copy()\n",
    "\n",
    "    if len(imageA.shape) == 3 and imageA.shape[-1] == 1:\n",
    "      imageA = imageA[:, :, 0]\n",
    "      imageB = imageB[:, :, 0]\n",
    "    \n",
    "    elif len(imageA.shape) == 4 and imageA.shape[-1] == 1:\n",
    "      imageA = imageA[:, :, :, 0]\n",
    "      imageB = imageB[:, :, :, 0]\n",
    "    \n",
    "    imageA = np.expand_dims(imageA, axis=0)\n",
    "    imageB = np.expand_dims(imageB, axis=0)\n",
    "    \n",
    "    # Use our siamese model to make predictions on the image pair, indicating whether or not the images belong to the same class\n",
    "    preds = model_visualize.predict([imageA, imageB])\n",
    "    proba = preds[0][0]\n",
    "\n",
    "    # Initialize the figure\n",
    "    fig = plt.figure(\"Pair #{}\".format(i + 1), figsize=(4, 2))\n",
    "    if model_visualize.loss == \"binary_crossentropy\":\n",
    "      plt.suptitle(\"Similarity: {:.2f}\".format(proba))\n",
    "    else:\n",
    "      plt.suptitle(\"Distance: {:.2f}\".format(proba))\n",
    "    \n",
    "    # Show first image\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(origA)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Show the second image\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(origB)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "     \n",
    "\n",
    "def get_recommendation(model, target, data, rec_number, printable=True):\n",
    "  recommended_list = []\n",
    "\n",
    "  origA = target.copy()\n",
    "  target = np.expand_dims(target, axis=0)\n",
    "\n",
    "  if len(origA.shape) == 3 and origA.shape[-1] == 1:\n",
    "      origA = origA[:, :, 0]\n",
    "\n",
    "  if len(origA.shape) == 4 and origA.shape[-1] == 1:\n",
    "      origA = origA[:, :, :, 0]\n",
    "\n",
    "  index = 0\n",
    "  for img in data:\n",
    "    \n",
    "    # Load both the images and convert them to grayscale create a copy of both the images for visualization purpose\n",
    "    origB = img.copy()\n",
    "\n",
    "    if len(origB.shape) == 3 and origB.shape[-1] == 1:\n",
    "      origB = origB[:, :, 0]\n",
    "\n",
    "    if len(origB.shape) == 4 and origB.shape[-1] == 1:\n",
    "      origB = origB[:, :, :, 0]\n",
    "    \n",
    "    \n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    # Use our siamese model to make predictions on the image pair, indicating whether or not the images belong to the same class\n",
    "    preds = model.predict([target, img], verbose = 0)\n",
    "    proba = preds[0][0]\n",
    "\n",
    "    recommended_list.append((origB, proba, index))\n",
    "    index = index + 1\n",
    "  \n",
    " \n",
    "  if model.loss == \"binary_crossentropy\":\n",
    "    \n",
    "    # Better greater similarity\n",
    "    recommended_list.sort(key=lambda a: a[1], reverse=True)\n",
    "  else: \n",
    "    \n",
    "    # Better less distance\n",
    "    recommended_list.sort(key=lambda a: a[1])\n",
    "\n",
    "  count = 0\n",
    "  recommended_list_index = []\n",
    "  for (rec_img, rec_pred, aux_index) in recommended_list:\n",
    "    if count == rec_number:\n",
    "      break\n",
    "    \n",
    "    # Get the index of the recommended item in original list\n",
    "    recommended_list_index.append(aux_index)\n",
    "\n",
    "    if printable:\n",
    "      \n",
    "      # Initialize the figure\n",
    "      fig = plt.figure(\"Recommendations\", figsize=(4, 2))\n",
    "      if model.loss == \"binary_crossentropy\":\n",
    "        plt.suptitle(\"Similarity: {:.2f}\".format(rec_pred))\n",
    "      else:\n",
    "        plt.suptitle(\"Distance: {:.2f}\".format(rec_pred))\n",
    "      \n",
    "      # Show first image\n",
    "      ax = fig.add_subplot(1, 2, 1)\n",
    "      plt.imshow(origA)\n",
    "      plt.axis(\"off\")\n",
    "      \n",
    "      # Show the second image\n",
    "      ax = fig.add_subplot(1, 2, 2)\n",
    "      plt.imshow(rec_img)\n",
    "      plt.axis(\"off\")\n",
    "      \n",
    "      # Show the plot\n",
    "      plt.show()\n",
    "    count += 1\n",
    "\n",
    "  return recommended_list_index\n",
    "     \n",
    "\n",
    "def recall_and_precission_at_k(model, images, labels, k, p_groups=0.05):\n",
    "  n_items = len(images)\n",
    "\n",
    "  np.array(images)\n",
    "  images = images / 255.0\n",
    "  images = np.expand_dims(images, axis=-1)\n",
    "\n",
    "  items_index = random.sample(range(1, n_items), int(n_items*p_groups))\n",
    "  \n",
    "  sum_recall_list = []\n",
    "  sum_precission_list = []\n",
    "\n",
    "  for index in items_index:\n",
    "    \n",
    "    # The data retrieved is not given to de the net\n",
    "    target = images[index]\n",
    "    target_label = labels[index]\n",
    "    data = np.concatenate((images[:index], images[index+1:]), axis=0)\n",
    "    data_labels = np.concatenate((labels[:index], labels[index+1:]), axis=0)\n",
    "\n",
    "    # Get top recommendations\n",
    "    top_index = get_recommendation(model, target, data, k, printable=False)\n",
    "    top_labels = data_labels[top_index].tolist()\n",
    "\n",
    "    # Compute Rel_k/Rel\n",
    "    data_labels = data_labels.tolist()\n",
    "    rel = data_labels.count(target_label)\n",
    "\n",
    "    rel_at_k = top_labels.count(target_label)\n",
    "    sum_recall_list.append(rel_at_k / rel)\n",
    "\n",
    "    # Compute Rel_k/k\n",
    "    sum_precission_list.append(rel_at_k / k)\n",
    "\n",
    "  return sum(sum_recall_list) / len(sum_recall_list), sum(sum_precission_list) / len(sum_precission_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "rawDataPath = '../../Data/Furniture_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing ../../Data/Furniture_Data/lamps\\Modern\\11286modern-lighting.jpg: [Errno 13] Permission denied: '../../Data/Furniture_Data/lamps\\\\Modern\\\\11286modern-lighting.jpg'\n"
     ]
    }
   ],
   "source": [
    "# Load DataFrame\n",
    "furniture_dataset = load(rawDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImgPath</th>\n",
       "      <th>FileType</th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "      <th>Ratio</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Bands</th>\n",
       "      <th>Transparency</th>\n",
       "      <th>Animated</th>\n",
       "      <th>Category</th>\n",
       "      <th>Interior_Style</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beds\\Asian\\19726asian-daybeds.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beds\\Asian\\20027asian-canopy-beds.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beds\\Asian\\20109asian-panel-beds.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beds\\Asian\\20508asian-platform-beds.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beds\\Asian\\20750asian-comforters-and-comforter...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90078</th>\n",
       "      <td>tables\\Victorian\\5victorian-side-tables-and-en...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90079</th>\n",
       "      <td>tables\\Victorian\\6victorian-side-tables-and-en...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90080</th>\n",
       "      <td>tables\\Victorian\\7victorian-side-tables-and-en...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90081</th>\n",
       "      <td>tables\\Victorian\\8victorian-dining-tables.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90082</th>\n",
       "      <td>tables\\Victorian\\9victorian-side-tables-and-en...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90083 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ImgPath FileType  Width  \\\n",
       "0                      beds\\Asian\\19726asian-daybeds.jpg      jpg    350   \n",
       "1                  beds\\Asian\\20027asian-canopy-beds.jpg      jpg    350   \n",
       "2                   beds\\Asian\\20109asian-panel-beds.jpg      jpg    350   \n",
       "3                beds\\Asian\\20508asian-platform-beds.jpg      jpg    350   \n",
       "4      beds\\Asian\\20750asian-comforters-and-comforter...      jpg    350   \n",
       "...                                                  ...      ...    ...   \n",
       "90078  tables\\Victorian\\5victorian-side-tables-and-en...      jpg    350   \n",
       "90079  tables\\Victorian\\6victorian-side-tables-and-en...      jpg    350   \n",
       "90080  tables\\Victorian\\7victorian-side-tables-and-en...      jpg    350   \n",
       "90081      tables\\Victorian\\8victorian-dining-tables.jpg      jpg    350   \n",
       "90082  tables\\Victorian\\9victorian-side-tables-and-en...      jpg    350   \n",
       "\n",
       "       Height  Ratio Mode  Bands  Transparency  Animated Category  \\\n",
       "0         350    1.0  RGB  R G B         False     False     beds   \n",
       "1         350    1.0  RGB  R G B         False     False     beds   \n",
       "2         350    1.0  RGB  R G B         False     False     beds   \n",
       "3         350    1.0  RGB  R G B         False     False     beds   \n",
       "4         350    1.0  RGB  R G B         False     False     beds   \n",
       "...       ...    ...  ...    ...           ...       ...      ...   \n",
       "90078     350    1.0  RGB  R G B         False     False   tables   \n",
       "90079     350    1.0  RGB  R G B         False     False   tables   \n",
       "90080     350    1.0  RGB  R G B         False     False   tables   \n",
       "90081     350    1.0  RGB  R G B         False     False   tables   \n",
       "90082     350    1.0  RGB  R G B         False     False   tables   \n",
       "\n",
       "      Interior_Style  \n",
       "0              Asian  \n",
       "1              Asian  \n",
       "2              Asian  \n",
       "3              Asian  \n",
       "4              Asian  \n",
       "...              ...  \n",
       "90078      Victorian  \n",
       "90079      Victorian  \n",
       "90080      Victorian  \n",
       "90081      Victorian  \n",
       "90082      Victorian  \n",
       "\n",
       "[90083 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "furniture_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                    beds\\Asian\\19726asian-daybeds.jpg\n",
      "1                beds\\Asian\\20027asian-canopy-beds.jpg\n",
      "2                 beds\\Asian\\20109asian-panel-beds.jpg\n",
      "3              beds\\Asian\\20508asian-platform-beds.jpg\n",
      "4    beds\\Asian\\20750asian-comforters-and-comforter...\n",
      "Name: ImgPath, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(furniture_dataset[\"ImgPath\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing ../../Data/Furniture_Data/lamps\\Modern\\11286modern-lighting.jpg: Expected one match for lamps\\Modern\\11286modern-lighting.jpg, but found 0\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.6 GiB for an array with shape (90083, 224, 224, 3) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-ac9221ba0735>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_pairs_route\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawDataPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfurniture_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMG_SHAPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-1e0cbef46b74>\u001b[0m in \u001b[0;36mget_pairs_route\u001b[1;34m(images_dir, data_df, image_shape)\u001b[0m\n\u001b[0;32m     85\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Error processing {img_path}: {e}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\GPU_env\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[0msl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[0mexpanded_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 12.6 GiB for an array with shape (90083, 224, 224, 3) and data type uint8"
     ]
    }
   ],
   "source": [
    "images, labels, names = get_pairs_route(rawDataPath, furniture_dataset, IMG_SHAPE)\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, None, None, ..., None, None, None], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
