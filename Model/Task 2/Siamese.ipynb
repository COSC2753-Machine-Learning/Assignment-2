{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore Warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import os, sys\n",
    "import torch\n",
    "import torchvision\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Add the module path to the sys.path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local helper files\n",
    "from utils.dataset import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CUDA devices: 1\n",
      "Device 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available CUDA devices: {num_gpus}\")\n",
    "    \n",
    "    # Print the name of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "# Set the device\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the shape of the inputs for our network\n",
    "IMG_SIZE = 120\n",
    "CROP_SIZE = 110\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the batch size, number of epochs, and the size of the chunks\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "CHUNK_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predefined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair Dataset\n",
    "class FurniturePairDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        self.data_df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.color_histograms = []\n",
    "        self.edge_histograms = []\n",
    "        self._load_images_and_features()\n",
    "\n",
    "    def _load_images_and_features(self):\n",
    "        for idx in range(len(self.data_df)):\n",
    "            try: \n",
    "                img_name = os.path.join(self.root_dir, self.data_df.iloc[idx, 0])\n",
    "                image = Image.open(img_name)\n",
    "                self.images.append(image)\n",
    "                self.color_histograms.append(self._extract_color_histogram(image))\n",
    "                self.edge_histograms.append(self._extract_edge_histogram(image))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "\n",
    "    def _extract_color_histogram(self, image):\n",
    "        image = np.array(image.convert(\"RGB\"))\n",
    "        hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "        cv2.normalize(hist, hist)\n",
    "        return hist.flatten()\n",
    "\n",
    "    def _extract_edge_histogram(self, image):\n",
    "        image = np.array(image.convert(\"L\"))\n",
    "        edges = cv2.Canny(image, 100, 200)\n",
    "        hist = cv2.calcHist([edges], [0], None, [256], [0, 256])\n",
    "        cv2.normalize(hist, hist)\n",
    "        return hist.flatten()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df) * 2  # Each image will appear in one positive and one negative pair\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_idx = idx // 2\n",
    "        anchor_image = self.images[anchor_idx]\n",
    "        anchor_color_hist = self.color_histograms[anchor_idx]\n",
    "        anchor_edge_hist = self.edge_histograms[anchor_idx]\n",
    "\n",
    "        if self.transform:\n",
    "            anchor_image = self.transform(anchor_image)\n",
    "\n",
    "        if idx % 2 == 0:\n",
    "            similar_idx = self._find_most_similar(anchor_idx, anchor_color_hist)\n",
    "            similar_image = self.images[similar_idx]\n",
    "\n",
    "            if self.transform:\n",
    "                similar_image = self.transform(similar_image)\n",
    "\n",
    "            return (anchor_image, similar_image), 1\n",
    "        else:\n",
    "            dissimilar_idx = self._find_most_dissimilar(anchor_idx, anchor_edge_hist)\n",
    "            dissimilar_image = self.images[dissimilar_idx]\n",
    "\n",
    "            if self.transform:\n",
    "                dissimilar_image = self.transform(dissimilar_image)\n",
    "\n",
    "            return (anchor_image, dissimilar_image), 0\n",
    "\n",
    "    def _find_most_similar(self, anchor_idx, anchor_hist):\n",
    "        max_similarity = -1\n",
    "        similar_idx = -1\n",
    "        for idx in range(len(self.data_df)):\n",
    "            if idx == anchor_idx:\n",
    "                continue\n",
    "            hist = self.color_histograms[idx]\n",
    "            similarity = cv2.compareHist(anchor_hist, hist, cv2.HISTCMP_CORREL)\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                similar_idx = idx\n",
    "        return similar_idx\n",
    "\n",
    "    def _find_most_dissimilar(self, anchor_idx, anchor_hist):\n",
    "        min_similarity = 1\n",
    "        dissimilar_idx = -1\n",
    "        for idx in range(len(self.data_df)):\n",
    "            if idx == anchor_idx:\n",
    "                continue\n",
    "            hist = self.edge_histograms[idx]\n",
    "            similarity = cv2.compareHist(anchor_hist, hist, cv2.HISTCMP_CORREL)\n",
    "            if similarity < min_similarity:\n",
    "                min_similarity = similarity\n",
    "                dissimilar_idx = idx\n",
    "        return dissimilar_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "dataPath = '../../Data/Furniture_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing ../../Data/Furniture_Data/lamps\\Modern\\11286modern-lighting.jpg: [Errno 13] Permission denied: '../../Data/Furniture_Data/lamps\\\\Modern\\\\11286modern-lighting.jpg'\n"
     ]
    }
   ],
   "source": [
    "# Load DataFrame\n",
    "furniture_dataset = load(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImgPath</th>\n",
       "      <th>FileType</th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "      <th>Ratio</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Bands</th>\n",
       "      <th>Transparency</th>\n",
       "      <th>Animated</th>\n",
       "      <th>Category</th>\n",
       "      <th>Interior_Style</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beds\\Asian\\19726asian-daybeds.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beds\\Asian\\20027asian-canopy-beds.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beds\\Asian\\20109asian-panel-beds.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beds\\Asian\\20508asian-platform-beds.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beds\\Asian\\20750asian-comforters-and-comforter...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beds</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90078</th>\n",
       "      <td>tables\\Victorian\\5victorian-side-tables-and-en...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90079</th>\n",
       "      <td>tables\\Victorian\\6victorian-side-tables-and-en...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90080</th>\n",
       "      <td>tables\\Victorian\\7victorian-side-tables-and-en...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90081</th>\n",
       "      <td>tables\\Victorian\\8victorian-dining-tables.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90082</th>\n",
       "      <td>tables\\Victorian\\9victorian-side-tables-and-en...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RGB</td>\n",
       "      <td>R G B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tables</td>\n",
       "      <td>Victorian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90083 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ImgPath FileType  Width  \\\n",
       "0                      beds\\Asian\\19726asian-daybeds.jpg      jpg    350   \n",
       "1                  beds\\Asian\\20027asian-canopy-beds.jpg      jpg    350   \n",
       "2                   beds\\Asian\\20109asian-panel-beds.jpg      jpg    350   \n",
       "3                beds\\Asian\\20508asian-platform-beds.jpg      jpg    350   \n",
       "4      beds\\Asian\\20750asian-comforters-and-comforter...      jpg    350   \n",
       "...                                                  ...      ...    ...   \n",
       "90078  tables\\Victorian\\5victorian-side-tables-and-en...      jpg    350   \n",
       "90079  tables\\Victorian\\6victorian-side-tables-and-en...      jpg    350   \n",
       "90080  tables\\Victorian\\7victorian-side-tables-and-en...      jpg    350   \n",
       "90081      tables\\Victorian\\8victorian-dining-tables.jpg      jpg    350   \n",
       "90082  tables\\Victorian\\9victorian-side-tables-and-en...      jpg    350   \n",
       "\n",
       "       Height  Ratio Mode  Bands  Transparency  Animated Category  \\\n",
       "0         350    1.0  RGB  R G B         False     False     beds   \n",
       "1         350    1.0  RGB  R G B         False     False     beds   \n",
       "2         350    1.0  RGB  R G B         False     False     beds   \n",
       "3         350    1.0  RGB  R G B         False     False     beds   \n",
       "4         350    1.0  RGB  R G B         False     False     beds   \n",
       "...       ...    ...  ...    ...           ...       ...      ...   \n",
       "90078     350    1.0  RGB  R G B         False     False   tables   \n",
       "90079     350    1.0  RGB  R G B         False     False   tables   \n",
       "90080     350    1.0  RGB  R G B         False     False   tables   \n",
       "90081     350    1.0  RGB  R G B         False     False   tables   \n",
       "90082     350    1.0  RGB  R G B         False     False   tables   \n",
       "\n",
       "      Interior_Style  \n",
       "0              Asian  \n",
       "1              Asian  \n",
       "2              Asian  \n",
       "3              Asian  \n",
       "4              Asian  \n",
       "...              ...  \n",
       "90078      Victorian  \n",
       "90079      Victorian  \n",
       "90080      Victorian  \n",
       "90081      Victorian  \n",
       "90082      Victorian  \n",
       "\n",
       "[90083 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the dataset\n",
    "furniture_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation logic\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((120, 120)),\n",
    "    torchvision.transforms.RandomCrop((110, 110)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Siamese Network using PyTorch\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, embedding_dim=48):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=2, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=2, padding=1)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(64, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(-1, 64)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Euclidean distance layer\n",
    "class EuclideanDistance(nn.Module):\n",
    "    def forward(self, featsA, featsB):\n",
    "        return F.pairwise_distance(featsA, featsB, keepdim=True)\n",
    "\n",
    "# Define the complete Siamese Network model\n",
    "class SiameseModel(nn.Module):\n",
    "    def __init__(self, input_shape, embedding_dim=48):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.feature_extractor = SiameseNetwork(input_shape, embedding_dim)\n",
    "        self.euclidean_distance = EuclideanDistance()\n",
    "        self.fc = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, inputA, inputB):\n",
    "        featsA = self.feature_extractor(inputA)\n",
    "        featsB = self.feature_extractor(inputB)\n",
    "        distance = self.euclidean_distance(featsA, featsB)\n",
    "        output = torch.sigmoid(self.fc(distance))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = SiameseModel(IMG_SHAPE).to(DEVICE)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       factor=0.1,\n",
    "                                                       mode='max',\n",
    "                                                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a chunk for training\n",
    "def process_chunk_train(chunk_df, epoch, chunk_idx, num_chunks):\n",
    "    # Create the pair dataset for the chunk\n",
    "    pair_dataset = FurniturePairDataset(df=chunk_df, root_dir=dataPath, transform=transform)\n",
    "\n",
    "    # Create the pair dataloader for the chunk\n",
    "    pair_dataloader = DataLoader(pair_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Training loop for the chunk\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    total_batches = len(pair_dataloader)\n",
    "    print(f\"Total samples in this chunk: {len(chunk_df)}\")\n",
    "    print(f\"Total batches in this chunk: {total_batches}\")\n",
    "    for batch_idx, ((imageA, imageB), labels) in enumerate(pair_dataloader):\n",
    "        imageA, imageB, labels = imageA.cuda(), imageB.cuda(), labels.float().cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imageA, imageB)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Chunk {chunk_idx + 1}/{num_chunks}, Batch {batch_idx + 1}/{total_batches}, Loss: {running_loss / (batch_idx + 1)}\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Chunk {chunk_idx + 1}/{num_chunks} completed in {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training chunk 1/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 157\n",
      "Epoch 1/20, Chunk 1/10, Batch 50/157, Loss: 0.6930380201339722\n",
      "Epoch 1/20, Chunk 1/10, Batch 100/157, Loss: 0.6930230408906937\n",
      "Epoch 1/20, Chunk 1/10, Batch 150/157, Loss: 0.6930201248327891\n",
      "Epoch 1/20, Chunk 1/10 completed in 440.61 seconds\n",
      "Processing training chunk 2/10\n",
      "Total samples in this chunk: 10000\n",
      "Total batches in this chunk: 157\n",
      "Epoch 1/20, Chunk 2/10, Batch 50/157, Loss: 0.692973656654358\n",
      "Epoch 1/20, Chunk 2/10, Batch 100/157, Loss: 0.6929823559522629\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-e7791e0f6ba2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mchunk_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfurniture_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchunk_idx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mCHUNK_SIZE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_idx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mCHUNK_SIZE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Processing training chunk {chunk_idx + 1}/{num_chunks}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mprocess_chunk_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_chunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-4a8c761990a8>\u001b[0m in \u001b[0;36mprocess_chunk_train\u001b[1;34m(chunk_df, epoch, chunk_idx, num_chunks)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Total samples in this chunk: {len(chunk_df)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Total batches in this chunk: {total_batches}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimageA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimageB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpair_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mimageA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimageB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimageA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimageB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Minh\\Anaconda\\envs\\GPU_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Minh\\Anaconda\\envs\\GPU_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Minh\\Anaconda\\envs\\GPU_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Minh\\Anaconda\\envs\\GPU_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-73f3fbdbbca4>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 \u001b[0msimilar_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilar_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0manchor_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilar_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Minh\\Anaconda\\envs\\GPU_env\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Minh\\Anaconda\\envs\\GPU_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Minh\\Anaconda\\envs\\GPU_env\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \"\"\"\n\u001b[1;32m--> 304\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Minh\\Anaconda\\envs\\GPU_env\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    417\u001b[0m             )\n\u001b[0;32m    418\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Minh\\Anaconda\\envs\\GPU_env\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size)\u001b[0m\n\u001b[0;32m    263\u001b[0m                 \u001b[1;34m\"i.e. size should be an int or a sequence of length 1 in torchscript mode.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m             )\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Minh\\Anaconda\\envs\\GPU_env\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   1999\u001b[0m                 )\n\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2001\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2002\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2003\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process the dataset in chunks for training\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    num_chunks = len(furniture_dataset) // CHUNK_SIZE + 1\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        chunk_df = furniture_dataset.iloc[chunk_idx * CHUNK_SIZE:(chunk_idx + 1) * CHUNK_SIZE]\n",
    "        print(f\"Processing training chunk {chunk_idx + 1}/{num_chunks}\")\n",
    "        process_chunk_train(chunk_df, epoch, chunk_idx, num_chunks)\n",
    "    scheduler.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a chunk for testing\n",
    "def process_chunk_test(chunk_df):\n",
    "    # Create the pair dataset for the chunk\n",
    "    pair_dataset = FurniturePairDataset(df=chunk_df, root_dir=dataPath, transform=transform)\n",
    "\n",
    "    # Create the pair dataloader for the chunk\n",
    "    pair_dataloader = DataLoader(pair_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Testing loop for the chunk\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    total_batches = len(pair_dataloader)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, ((imageA, imageB), labels) in enumerate(pair_dataloader):\n",
    "            imageA, imageB, labels = imageA.cuda(), imageB.cuda(), labels.float().cuda()\n",
    "            outputs = model(imageA, imageB)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            running_loss += loss.item()\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Chunk Test completed in {elapsed_time:.2f} seconds, Test Loss: {running_loss/total_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset in chunks for testing\n",
    "num_chunks = len(furniture_dataset) // CHUNK_SIZE + 1\n",
    "for chunk_idx in range(num_chunks):\n",
    "    chunk_df = furniture_dataset.iloc[chunk_idx * CHUNK_SIZE:(chunk_idx + 1) * CHUNK_SIZE]\n",
    "    print(f\"Processing test chunk {chunk_idx + 1}/{num_chunks}\")\n",
    "    process_chunk_test(chunk_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
